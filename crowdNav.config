[train_config]

[test_config]
; the model directory that we are testing
model_directory = trained_models/env_experiment
; render the environment or not
visualize = True
; if -1, it will run 500 different cases; if >=0, it will run the specified test case repeatedly
test_case= -1
; model weight file you want to test
model_weight_file = 41665.pt
; whether to save trajectories of episodes
render_traj = False
; whether to save slide show of episodes
save_slides = False
; verbose mode
verbose = True

[algorithm_config]
; the saving directory for train.py
output_directory = trained_models/env_experiment
; resume training from an existing checkpoint or not
resume_training = False
; if resume = True, load from the following checkpoint
load-path = trained_models/env_experiment/checkpoints/41200.pt
; whether to overwrite the output directory in training
overwrite = True
; number of threads used for intraop parallelism on CPU
num_threads = 1
; only implement in testing
phase = test
; sets flags for determinism when using CUDA (potentially slow!)
cuda_deterministic = False
; disables CUDA training, only works for gpu only (although you can make it work on cpu after some minor fixes)
no_cuda = True
; sets the seed for generating random numbers, -1 for random seed
seed = 425
; how many training processes to use 
num-processes = 16
; number of batches for ppo 
num_mini_batch = 32
; number of forward steps in A2C 
num_steps = 5
; use a recurrent policy 
recurrent_policy = True
; number of ppo epochs
ppo-epoch = 5
; ppo clip parameter
clip-param = 0.2
; value loss coefficient
value-loss-coef = 0.5
; entropy term coefficient
entropy-coef = 0.0
; learning rate
lr = 4e-5
; RMSprop optimizer epsilon
eps = 1e-5
; RMSprop optimizer apha
alpha = 0.99
; discount factor for rewards
gamma = 0.99
; max norm of gradients
max_grad_norm =0.5
; number of environment steps to train, 10e6 for holonomic, 20e6 for unicycle
num_env_steps = 20e6,
; use a linear schedule on the learning rate, True for unicycle, False for holonomic
use_linear_lr_decay =True
; algorithm to use: a2c | ppo | acktr
algo = ppo
; save interval, one save per n updates
save_interval =200
; use generalized advantage estimation
use_gae = True
; gae lambda parameter
gae_lambda = 0.95,
; log interval, one log per n updates
log_interval = 20
; compute returns taking into account time limits
use_proper_time_limits = False,
; for srnn only
; Size of Human Node RNN hidden state
human_node_rnn_size = 128
; Size of Human Human Edge RNN hidden state
human_human_edge_rnn_size = 256
; Auxiliary loss on human nodes outputs
aux_loss = False 
; Dimension of the node features
human_node_input_size =3
; Dimension of the edge features
human_human_edge_input_size = 2
; Dimension of the node output
human_node_output_size = 256
; Embedding size of node features
human_node_embedding_size = 64
; Embedding size of edge features
human_human_edge_embedding_size = 64
; Attention size
attention_size = 64
; Sequence length
seq_length = 30
; use self attn in human states or not
use_self_attn =True
; use attn between humans and robots or not (todo: implment this in network models)
use_hr_attn = True
; No prediction: for orca, sf, old_srnn, selfAttn_srnn_noPred ablation: 'CrowdSimVarNum-v0',
; for constant velocity Pred, ground truth Pred: 'CrowdSimPred-v0'
; gst pred: 'CrowdSimPredRealGST-v0'
; name of the environment
env_name = CrowdSimPredRealGST-v0
; sort all humans and squeeze them to the front or not
sort_humans = True
[environment_config]